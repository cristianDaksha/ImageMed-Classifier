{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_recall_curve, average_precision_score, PrecisionRecallDisplay, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "pwd_notebook = os.path.abspath('') # path notebook\n",
    "root_path = os.path.dirname(os.path.dirname(pwd_notebook)) #path root project\n",
    "\n",
    "data_dir = os.path.join(root_path, 'data') #path data\n",
    "\n",
    "# train path\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "print(train_dir)\n",
    "# test path\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "print(test_dir)\n",
    "# valid path\n",
    "valid_dir = os.path.join(data_dir, 'valid')\n",
    "print(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "#Add absolute path of the checkpoint\n",
    "checkpoint_path = 'C:\\\\User\\\\Documents\\\\brahma_project\\\\checkpoints\\\\arquitecture_distorsion\\\\resnet50_architecture_distorsion\\\\RUN_20240404_154343\\\\ckpt_best.pth'\n",
    "\n",
    "n_classes = 4 #Add number of classes\n",
    "\n",
    "#Add name of pre-trained weights\n",
    "name_pretrained_weights = 'imagenet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.get(model_name=Models.RESNET50, #Add model name\n",
    "                   num_classes=n_classes, \n",
    "                   pretrained_weights=name_pretrained_weights, \n",
    "                   checkpoint_num_classes=n_classes, \n",
    "                   checkpoint_path=checkpoint_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .pth file\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Check if the key 'state_dict' or some similar key exists\n",
    "if 'state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "elif 'net' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['net'])\n",
    "else:\n",
    "    # Whether the .pth file directly contains the model state\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines transformations if they are not defined\n",
    "test_transform = Compose([\n",
    "    # Here are the same transformations that you used for training\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load test data\n",
    "test_data = datasets.ImageFolder(root=test_dir, transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, \n",
    "                             batch_size=16, # Adjust as needed\n",
    "                             num_workers=2, # Adjust based on your available resources\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an image from the test set (replace 'index' with the index of the image you want)\n",
    "index = 2  # Change this to the desired image index\n",
    "image, true_label = test_data[index]  # Get the image and its true label\n",
    "\n",
    "# Perform model inference on the image\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    image = image.unsqueeze(0)  # Add a batch dimension since the model expects batches\n",
    "    output = model(image.to(device))\n",
    "     # Convert logits to probabilities\n",
    "    probabilities = F.softmax(output, dim=1)\n",
    "    _, predicted_label = torch.max(output, 1)  # Get the predicted label\n",
    "    predicted_score = probabilities[0][predicted_label.item()].item()  # Get the probability of the predicted label\n",
    "\n",
    "# Get the name of the predicted class from the index\n",
    "predicted_class = test_data.classes[predicted_label.item()]\n",
    "true_class = test_data.classes[true_label]\n",
    "\n",
    "\n",
    "# Show image along with predicted and true label\n",
    "plt.imshow(image.squeeze().permute(1, 2, 0))  # Show image (make sure dimensions are appropriate)\n",
    "plt.title(f\"Predicted: {predicted_class} \\n Score: {predicted_score*100:.2f}% \\nTrue: {true_class}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Df inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "# Create table with the results de test (true, predict, score and path image)\n",
    "results = []\n",
    "for i, (image, true_label) in enumerate(test_data):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0)  # Add a batch dimension since the model expects batches\n",
    "        output = model(image.to(device))\n",
    "        # Convert logits to probabilities\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        _, predicted_label = torch.max(output, 1)  # Get the predicted label\n",
    "        predicted_score = probabilities[0][predicted_label.item()].item()  # Get the probability of the predicted label\n",
    "        # pass predicted_score to percentage with 2 decimals\n",
    "        predicted_score = \"{:.2f}\".format(predicted_score*100)\n",
    "    # Get the name of the predicted class from the index\n",
    "    predicted_class = test_data.classes[predicted_label.item()]\n",
    "    true_class = test_data.classes[true_label]\n",
    "        \n",
    "    # Add new column with \"X\" where the true class is equal to the predicted class\n",
    "    if true_class == predicted_class:\n",
    "        true_true = ' (X)'\n",
    "    else:\n",
    "        true_true = ' ( )'\n",
    "    \n",
    "    # in the path image add name folder and image and not path complete\n",
    "    # results.append([true_class, predicted_class, predicted_score, test_data.imgs[i][0].split('\\\\')[-2] + '/' + test_data.imgs[i][0].split('\\\\')[-1]])    \n",
    "    \n",
    "    # in the path image add path complete\n",
    "    results.append([true_class, predicted_class, predicted_score, test_data.imgs[i][0], true_true])\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results, columns=['True', 'Predicted', 'Score', 'Image Path', 'Acertado'])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate the model on the test data set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate general metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "report = classification_report(all_labels, all_preds, target_names=test_data.classes)\n",
    "\n",
    "print(f\"\\n Accuracy: {accuracy}\\n\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', xticklabels=test_data.classes, yticklabels=test_data.classes, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain Probabilities of each Class\n",
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_scores.extend(probabilities)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_scores = np.array(y_scores)\n",
    "\n",
    "# Binarize tags\n",
    "y_true_bin = label_binarize(y_true, classes=np.arange(len(test_data.classes)))\n",
    "n_classes = len(test_data.classes)\n",
    "\n",
    "# Calculate average precision and recall for each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_scores[:, i])\n",
    "    average_precision[i] = average_precision_score(y_true_bin[:, i], y_scores[:, i])\n",
    "\n",
    "# Colors for different classes\n",
    "colors = cycle(['blue', 'red', 'green', 'yellow', 'orange', 'purple'])\n",
    "\n",
    "# Draw Precision-Recall curves and iso-F1 curves\n",
    "plt.figure(figsize=(7, 8))\n",
    "f_scores = np.linspace(0.2, 0.8, num=6)\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    # l, = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2) # iso-F1 curves\n",
    "    plt.annotate(f\"f1={f_score:0.1f}\", xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "             label=f'Precision-Recall curve of {test_data.classes[i]} (AP = {average_precision[i]:0.2f})')\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Precision-Recall curve to multi-class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve || FPR vs TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize tags\n",
    "y_true_bin = label_binarize(y_true, classes=np.arange(len(test_data.classes)))\n",
    "n_classes = len(test_data.classes)\n",
    "\n",
    "# Calculate ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Colors for different classes\n",
    "colors = cycle(['blue', 'red', 'green', 'yellow', 'orange', 'purple'])\n",
    "\n",
    "# Draw the ROC curve for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'ROC curve of {test_data.classes[i]} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Draw line for classifier\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (Fall out)')\n",
    "plt.ylabel('True Positive Rate (Sensivity)')\n",
    "plt.title('ROC Curve for Multi-Class FPR vs TPR')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve || Specificty vs Sensibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize tags\n",
    "y_true_bin = label_binarize(y_true, classes=np.arange(len(test_data.classes)))\n",
    "n_classes = len(test_data.classes)\n",
    "\n",
    "# Colors for different classes\n",
    "colors = cycle(['blue', 'red', 'green', 'yellow', 'orange', 'purple'])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate ROC curve (TNR and TPR) and ROC area (AROC) for each class\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
    "    tnr = 1 - fpr\n",
    "    roc_auc = auc(tnr, tpr)\n",
    "\n",
    "    plt.plot(tnr, tpr, color=color, lw=2, label=f'{test_data.classes[i]} (AROC = {roc_auc:.2f})')\n",
    "\n",
    "plt.xlabel('True Negatives Rate (TNR - Specificity)')\n",
    "plt.ylabel('True Positive Rate (TPR - Sensibility)')\n",
    "plt.title('ROC Curve for Multi-Class Specificity vs Sensibility')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
