{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_recall_curve, average_precision_score, PrecisionRecallDisplay, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "pwd_notebook = os.path.abspath('') # path notebook\n",
    "root_path = os.path.dirname(os.path.dirname(pwd_notebook)) #path root project\n",
    "\n",
    "data_dir = os.path.join(root_path, 'data') #path data\n",
    "\n",
    "# train path\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "print(train_dir)\n",
    "# test path\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "print(test_dir)\n",
    "# valid path\n",
    "valid_dir = os.path.join(data_dir, 'valid')\n",
    "print(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get name from experiments\n",
    "def extraction(path):\n",
    "    match = re.search(r'([^/]+)/RUN_\\d{8}_\\d{6}_\\d+', path)\n",
    "    if match:\n",
    "        exp_name = match.group(1)\n",
    "        run_name = match.group(0).split('/')[-1]\n",
    "        return run_name, exp_name\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "checkpoint_path = '' #Add absolute path of the checkpoint\n",
    "\n",
    "\n",
    "run_name, exp_name = extraction(checkpoint_path)\n",
    "\n",
    "n_classes = 4 #Add number of classes\n",
    "\n",
    "#Add name of pre-trained weights\n",
    "name_pretrained_weights = 'imagenet'\n",
    "\n",
    "#Add size of image for resize\n",
    "size_image = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pararmeters from experiments file checkpoints\n",
    "\n",
    "def parameters (exps_path):\n",
    "    with open(exps_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    opt_p = r'\"optimizer\":\\s*\"([^\"]+)\"'\n",
    "    lr_p = r'\"initial_lr\":\\s*([\\d.]+)'\n",
    "    epochs_p = r'\"max_epochs\":\\s*(\\d+)'\n",
    "    device_p = r'\"device_type\":\\s*\"([^\"]+)\"'\n",
    "    \n",
    "    optimizer = re.search(opt_p, content)\n",
    "    initial_lr = re.search(lr_p, content)\n",
    "    max_epochs = re.search(epochs_p, content)\n",
    "    device_type = re.search(device_p, content)\n",
    "    \n",
    "    optimizer = optimizer.group(1) if optimizer else None\n",
    "    initial_lr = initial_lr.group(1) if initial_lr else None\n",
    "    max_epochs = max_epochs.group(1) if max_epochs else None\n",
    "    device_type = device_type.group(1) if device_type else None\n",
    "    \n",
    "    return {\n",
    "        \"optimizer\": optimizer,\n",
    "        \"initial_lr\": initial_lr,\n",
    "        \"max_epochs\": max_epochs,\n",
    "        \"device_type\": device_type,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.get(model_name=Models.RESNET50, #Add model name\n",
    "                   num_classes=n_classes, \n",
    "                   pretrained_weights=name_pretrained_weights, \n",
    "                   checkpoint_num_classes=n_classes, \n",
    "                   checkpoint_path=checkpoint_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .pth file\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Check if the key 'state_dict' or some similar key exists\n",
    "if 'state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "elif 'net' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['net'])\n",
    "else:\n",
    "    # Whether the .pth file directly contains the model state\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines transformations if they are not defined\n",
    "test_transform = Compose([\n",
    "    # Here are the same transformations that you used for training\n",
    "    transforms.Resize((size_image)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load test data\n",
    "test_data = datasets.ImageFolder(root=test_dir, transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, \n",
    "                             batch_size=16, # Adjust as needed\n",
    "                             num_workers=2, # Adjust based on your available resources\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an image from the test set (replace 'index' with the index of the image you want)\n",
    "index = 9# Change this to the desired image index\n",
    "image, true_label = test_data[index]  # Get the image and its true label\n",
    "\n",
    "# Perform model inference on the image\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    image = image.unsqueeze(0)  # Add a batch dimension since the model expects batches\n",
    "    output = model(image.to(device))\n",
    "     # Convert logits to probabilities\n",
    "    probabilities = F.softmax(output, dim=1)\n",
    "    _, predicted_label = torch.max(output, 1)  # Get the predicted label\n",
    "    predicted_score = probabilities[0][predicted_label.item()].item()  # Get the probability of the predicted label\n",
    "\n",
    "# Get the name of the predicted class from the index\n",
    "predicted_class = test_data.classes[predicted_label.item()]\n",
    "true_class = test_data.classes[true_label]\n",
    "\n",
    "\n",
    "# Show image along with predicted and true label\n",
    "plt.imshow(image.squeeze().permute(1, 2, 0))  # Show image (make sure dimensions are appropriate)\n",
    "plt.title(f\"Predicted: {predicted_class} \\n Score: {predicted_score*100:.2f}% \\nTrue: {true_class}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Df inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "# Create table with the results de test (true, predict, score and path image)\n",
    "results = []\n",
    "for i, (image, true_label) in enumerate(test_data):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0)  # Add a batch dimension since the model expects batches\n",
    "        output = model(image.to(device))\n",
    "        # Convert logits to probabilities\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        _, predicted_label = torch.max(output, 1)  # Get the predicted label\n",
    "        predicted_score = probabilities[0][predicted_label.item()].item()  # Get the probability of the predicted label\n",
    "        # pass predicted_score to percentage with 2 decimals\n",
    "        predicted_score = \"{:.2f}\".format(predicted_score*100)\n",
    "    # Get the name of the predicted class from the index\n",
    "    predicted_class = test_data.classes[predicted_label.item()]\n",
    "    true_class = test_data.classes[true_label]\n",
    "        \n",
    "    # Add new column with \"X\" where the true class is equal to the predicted class\n",
    "    if true_class == predicted_class:\n",
    "        true_true = ' (X)'\n",
    "    else:\n",
    "        true_true = ' ( )'\n",
    "    \n",
    "    # in the path image add name folder and image and not path complete\n",
    "    # results.append([true_class, predicted_class, predicted_score, test_data.imgs[i][0].split('\\\\')[-2] + '/' + test_data.imgs[i][0].split('\\\\')[-1], true_true])    \n",
    "    \n",
    "    # in the path image add path complete\n",
    "    results.append([true_class, predicted_class, predicted_score, test_data.imgs[i][0], true_true])\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results, columns=['True', 'Predicted', 'Score', 'Image Path', 'Correct'])\n",
    "\n",
    "# Save the DataFrame to a XLS file\n",
    "results_df.to_excel(f'../../checkpoints/clasificador/{exp_name}/{run_name}/{run_name}.xlsx', index=False)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate the model on the test data set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate general metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "report = classification_report(all_labels, all_preds, target_names=test_data.classes, zero_division=0, digits=5)\n",
    "\n",
    "print(f\"\\n Accuracy: {accuracy}\\n\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', xticklabels=test_data.classes, yticklabels=test_data.classes, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain Probabilities of each Class\n",
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_scores.extend(probabilities)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_scores = np.array(y_scores)\n",
    "\n",
    "# Binarize tags\n",
    "y_true_bin = label_binarize(y_true, classes=np.arange(len(test_data.classes)))\n",
    "n_classes = len(test_data.classes)\n",
    "\n",
    "# Calculate average precision and recall for each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_scores[:, i])\n",
    "    average_precision[i] = average_precision_score(y_true_bin[:, i], y_scores[:, i])\n",
    "\n",
    "# Colors for different classes\n",
    "colors = cycle(['blue', 'red', 'green', 'yellow', 'orange', 'purple'])\n",
    "\n",
    "# Draw Precision-Recall curves and iso-F1 curves\n",
    "plt.figure(figsize=(7, 8))\n",
    "f_scores = np.linspace(0.2, 0.8, num=6)\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    # l, = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2) # iso-F1 curves\n",
    "    plt.annotate(f\"f1={f_score:0.1f}\", xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "             label=f'Precision-Recall curve of {test_data.classes[i]} (AP = {average_precision[i]:0.2f})')\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Precision-Recall curve to multi-class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve || FPR vs TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize tags\n",
    "y_true_bin = label_binarize(y_true, classes=np.arange(len(test_data.classes)))\n",
    "n_classes = len(test_data.classes)\n",
    "\n",
    "# Calculate ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Colors for different classes\n",
    "colors = cycle(['blue', 'red', 'green', 'yellow', 'orange', 'purple'])\n",
    "\n",
    "# Draw the ROC curve for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'ROC curve of {test_data.classes[i]} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Draw line for classifier\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (Fall out)')\n",
    "plt.ylabel('True Positive Rate (Sensivity)')\n",
    "plt.title('ROC Curve for Multi-Class FPR vs TPR')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve || Specificty vs Sensibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize tags\n",
    "y_true_bin = label_binarize(y_true, classes=np.arange(len(test_data.classes)))\n",
    "n_classes = len(test_data.classes)\n",
    "\n",
    "# Colors for different classes\n",
    "colors = cycle(['blue', 'red', 'green', 'yellow', 'orange', 'purple'])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate ROC curve (TNR and TPR) and ROC area (AROC) for each class\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
    "    tnr = 1 - fpr\n",
    "    roc_auc = auc(tnr, tpr)\n",
    "\n",
    "    plt.plot(tnr, tpr, color=color, lw=2, label=f'{test_data.classes[i]} (AROC = {roc_auc:.2f})')\n",
    "\n",
    "plt.xlabel('True Negatives Rate (TNR - Specificity)')\n",
    "plt.ylabel('True Positive Rate (TPR - Sensibility)')\n",
    "plt.title('ROC Curve for Multi-Class Specificity vs Sensibility')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct and incorrect prediction balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_path = \"\" # Add absolute path experiments logs\n",
    "info = parameters(exps_path)\n",
    "optimizer = info['optimizer']\n",
    "lr = info['initial_lr']\n",
    "epochs = info['max_epochs']\n",
    "device = info['device_type']\n",
    "print (f'Optimizer: {optimizer}\\nLr: {lr}\\nEpochs: {epochs}\\nDevice: {device}' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = results_df.copy()\n",
    "copy_df['Score'] = copy_df['Score'].astype(float)\n",
    "copy_df['Correct'] = copy_df['Correct'].apply(lambda x: 1 if x.strip() == '(X)' else 0)\n",
    "\n",
    "min_score = copy_df['Score'].min()\n",
    "max_score = copy_df['Score'].max()\n",
    "\n",
    "# Expand the range slightly to include edge cases\n",
    "range_expand = (max_score - min_score) * 0.01\n",
    "min_score -= range_expand\n",
    "max_score += range_expand\n",
    "\n",
    "# Create exactly 10 bins within the expanded range\n",
    "bins = pd.interval_range(start=min_score, end=max_score, periods=10, closed='right')\n",
    "\n",
    "# Assign each 'Score' to a bin\n",
    "copy_df['Score_bin'] = pd.cut(copy_df['Score'], bins, include_lowest=True)\n",
    "\n",
    "# Group by the bins and sum the 'Correct' values\n",
    "grouped = copy_df.groupby('Score_bin', observed=False)['Correct'].sum().reset_index()\n",
    "\n",
    "# Format the bin labels for the x-axis\n",
    "grouped['Score_bin'] = grouped['Score_bin'].apply(lambda x: f\"{x.left:.1f} - {x.right:.1f}\")\n",
    "\n",
    "y_max = 100  # Customize based on data\n",
    "\n",
    "# Plot the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Score_bin', y='Correct', data=grouped, color='blue')\n",
    "\n",
    "plt.xlabel('Score Range')\n",
    "plt.ylabel('Correct predictions')\n",
    "plt.ylim(0, y_max)\n",
    "plt.title(f'{run_name}, {epochs} epochs, lr {lr}, {optimizer} - Correct')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(False)  # Enable grid for better readability or don't\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df['Incorrect'] = copy_df['Correct'].apply(lambda x: 1 - x)\n",
    "grouped_incorrect = copy_df.groupby('Score_bin', observed=False)['Incorrect'].sum().reset_index()\n",
    "grouped_incorrect['Score_bin'] = grouped_incorrect['Score_bin'].apply(lambda x: f\"{x.left:.1f} - {x.right:.1f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Score_bin', y='Incorrect', data=grouped_incorrect, color='red')\n",
    "plt.xlabel('Score Range')\n",
    "plt.ylabel('Incorrect predictions')\n",
    "plt.ylim(0, y_max)  # Set the Y-axis maximum value\n",
    "plt.title(f'{run_name}, {epochs} epochs, lr {lr}, {optimizer} - Incorrect')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correct = copy_df['Correct'].sum()\n",
    "total_incorrect = copy_df['Incorrect'].sum()\n",
    "\n",
    "print(f\"Correct: {total_correct}\\nIncorrect: {total_incorrect}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
